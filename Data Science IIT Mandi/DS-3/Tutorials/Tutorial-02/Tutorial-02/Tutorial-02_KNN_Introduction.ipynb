{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## What is K Nearest Neighbour ?"],"metadata":{"id":"RHxCzcMw6yo1"}},{"cell_type":"markdown","source":["K-Nearest Neighbors (K-NN) is a supervised machine learning algorithm used for classification and regression tasks. It's a simple and intuitive algorithm that can be used for both classification and regression tasks."],"metadata":{"id":"itveBW6M69YO"}},{"cell_type":"markdown","source":["## How does K-NN works?"],"metadata":{"id":"HGblRMTr7pqC"}},{"cell_type":"markdown","source":["###Training Phase:\n","\n","In the training phase, K-NN doesn't do much training in the traditional sense. Instead, it stores the entire training dataset in memory. This dataset consists of labeled examples, where each example has a set of features and a corresponding class label (for classification) or a target value (for regression).\n"],"metadata":{"id":"ykpEghoi7HC8"}},{"cell_type":"markdown","source":["### Prediction Phase:\n","\n","When you want to make a prediction for a new, unseen data point, K-NN follows these steps:\n","\n","a. Calculate Distance:\n","\n","* Calculate the distance between the new data point and every data point in the training dataset. The most common distance metrics used are Euclidean distance, Manhattan distance, or Minkowski distance. The choice of distance metric depends on the problem and data.\n","\n","b. Select K Neighbors:\n","\n","* Choose the K data points with the shortest distances to the new data point. These K data points are referred to as the \"K nearest neighbors.\"\n","\n","c. Classification (for classification tasks):\n","\n","\n","*   For a classification task, K-NN counts the number of data points in each class among the K nearest neighbors.\n","*   The new data point is classified into the class that appears most frequently among the K neighbors. This is known as \"majority voting.\"\n","\n","d. Regression (for regression tasks):\n","\n","* For a regression task, K-NN calculates the average (or weighted average) of the target values of the K nearest neighbors.\n","* The new data point is assigned this average as its predicted target value.\n"],"metadata":{"id":"bcJJvtOn7PSy"}},{"cell_type":"markdown","source":["### Choosing K:\n","\n","One crucial hyperparameter in K-NN is the value of K. The choice of K can significantly impact the algorithm's performance. A smaller K value makes the algorithm more sensitive to noise in the data, potentially leading to overfitting, while a larger K value can make the decision boundary smoother but may result in underfitting."],"metadata":{"id":"NkX1uGxl7Q9N"}},{"cell_type":"markdown","source":["### Distance Metric:\n","\n","The choice of distance metric is another important consideration. Different distance metrics can lead to different results, so selecting an appropriate metric is essential. Common distance metrics include Euclidean distance, Manhattan distance, and others."],"metadata":{"id":"90iutZbH7TDs"}},{"cell_type":"markdown","source":["###Normalization:\n","\n","It's often recommended to normalize or standardize the features in your dataset before using K-NN. This ensures that features with different scales do not disproportionately influence the distance calculations."],"metadata":{"id":"CXqD5EQl7WOM"}},{"cell_type":"markdown","source":["## When to use KNN ?"],"metadata":{"id":"d5jQMxtl86-A"}},{"cell_type":"markdown","source":["K-Nearest Neighbors (K-NN) is a versatile algorithm that can be useful in certain situations. Here are some scenarios in which you might consider using K-NN:\n","\n","* Small to Moderate-Sized Datasets: K-NN can work well when you have a relatively small to moderately sized dataset. It doesn't require building an explicit model during training, so it's computationally feasible for datasets of this size.\n","\n","* No Assumptions About Data Distribution: K-NN doesn't make strong assumptions about the underlying data distribution. If you're unsure about the data distribution and whether it's linear or non-linear, K-NN can be a good choice as it can adapt to various data shapes.\n","\n","* Binary Classification: K-NN can perform well in binary classification tasks when you have two classes to distinguish between. It's relatively simple to implement and can be a good starting point for such problems.\n","\n","* Multi-Class Classification: K-NN can also be applied to multi-class classification problems, where you need to classify data points into more than two classes. However, the choice of K and distance metric becomes more critical in these cases.\n","\n","* Regression: K-NN can be used for regression tasks when you need to predict a continuous target variable. It calculates the average (or weighted average) of the target values of the K nearest neighbors to make predictions.\n","\n","* Localized Patterns: K-NN is particularly suitable for problems where the decision boundaries are highly localized. If the decision boundary varies in different parts of the feature space, K-NN can capture these localized patterns effectively.\n","\n","* Anomaly Detection: K-NN can be used for anomaly detection. Unusual data points may have few neighbors that are similar to them, so they can be detected as outliers.\n","\n","* Feature Engineering: K-NN can help identify important features in your dataset. By examining which features are most influential in determining the nearest neighbors, you can gain insights into the importance of various features.\n","\n","* Quick Prototyping: K-NN is easy to implement and can serve as a quick prototype for testing the feasibility of a machine learning solution on your data. It can be a starting point before exploring more complex algorithms"],"metadata":{"id":"Y8TXq9fS9DB4"}},{"cell_type":"markdown","source":["## When not to use KNN ?"],"metadata":{"id":"swA8PKi29Wr5"}},{"cell_type":"markdown","source":["* Large Datasets: K-NN can be computationally expensive, especially with large datasets, as it requires calculating distances between the new data point and all data points in the training set for each prediction.\n","\n","* High-Dimensional Data: K-NN is less effective in high-dimensional spaces because the concept of distance becomes less meaningful, and the curse of dimensionality can lead to increased computational complexity.\n","\n","* Imbalanced Datasets: If your dataset is heavily imbalanced (i.e., one class significantly outnumbers the others), K-NN can be biased towards the majority class. You might need to consider techniques like resampling or using different distance weights.\n","\n","* Optimal K Selection: Choosing the right value of K can be challenging. It often requires experimentation and cross-validation to determine the best K for your specific problem."],"metadata":{"id":"ayijjRco9ff4"}},{"cell_type":"code","source":["from IPython.display import Image\n","Image(url='https://miro.medium.com/v2/resize:fit:828/1*n9v1xsBi0bek98rqBnWGEg.gif')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":463},"id":"6QjqxoV0-K3Y","outputId":"8d55e697-19f2-4643-cf31-ed37c7e10b8f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"https://miro.medium.com/v2/resize:fit:828/1*n9v1xsBi0bek98rqBnWGEg.gif\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":[],"metadata":{"id":"8hgHeKP2-UZy"},"execution_count":null,"outputs":[]}]}