{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"The **k-nearest neighbors (KNN)** algorithm is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both **classification** and **regression** problems. The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other. \nKNN captures the idea of similarity (sometimes called distance, proximity, or closeness) with some mathematics we might have learned in our childhood— calculating the distance between points on a graph.\nThere are other ways of calculating distance, and one way might be preferable depending on the problem we are solving. However, the straight-line distance (also called the Euclidean distance) is a popular and familiar choice.","metadata":{}},{"cell_type":"markdown","source":"The data we will use for regression looks like mtcars data as a form. I have worked on artificial neural networks with mtcars data before.\n\n[Neural Network - Predict to Acceleration \"R Application\"](https://www.kaggle.com/hamzatanc/neural-network-arabalar-n-h-zlanmas-n-n-tahmini)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ncars_data = pd.read_csv(\"../input/car-data/car_data.csv\", index_col = \"car_name\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cars_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Mtcars data consists of 392 observations and 7 variables. Considering that the K nearest neighbors algorithm is successful in small data, we can ignore the small size of the data.","metadata":{}},{"cell_type":"code","source":"cars_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cars_data.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When I look at the distribution of data to quarters, accumulation did not attract my attention.","metadata":{}},{"cell_type":"code","source":"cars_data.groupby(\"cylinders\").count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In this study, I want to filter according to the number of cylinders. For this reason, I will use the data of the \"4 cylinder\" vehicles with the highest frequency.","metadata":{}},{"cell_type":"code","source":"cars_data = cars_data[cars_data.cylinders == 4]\ncars_data = cars_data.drop(\"cylinders\", axis = 1)\ncars_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cars_data.corr()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our aim in the study is to predict the **acceleration** variable with the KNN model. So \"acceleration\" is our **dependent** variable. We can actually determine the variables that affect the performance of vehicles from life experience. We can think of this experience as **\"Professional Knowledge\"**, which has an important place in data science. I used the correlation table while determining the independent variables along with the experience. When determining the independent variables, we should be careful that the correlations with the dependent variable are large and the correlations between the independent variables are small.","metadata":{}},{"cell_type":"markdown","source":"I select the variables which I will use in the K Nearest Neighbors Regression model as \"knn_regression_data\".","metadata":{}},{"cell_type":"code","source":"knn_regression_data = cars_data.loc[:,[\"horsepower\",\"weight\", \"mpg\",\"displacement\"]]\nknn_regression_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Normalization** is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to use a common scale, without distorting differences in the ranges of values or losing information. I use min/max normalizer. The min-max normalizer linearly rescales every feature to the [0,1] interval. Rescaling to the [0,1] interval is done by shifting the values of each feature so that the minimal value is 0, and then dividing by the new maximal value (which is the difference between the original maximal and minimal values).","metadata":{}},{"cell_type":"markdown","source":"![](http://bilgisayarkavramlari.sadievrenseker.com/wp-content/uploads/2012/01/normallesme6.png)","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\nknn_regression_data = (knn_regression_data - np.min(knn_regression_data))/(np.max(knn_regression_data) - np.min(knn_regression_data))\nknn_regression_data.describe().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we examine the distribution of our normalized data, we see that the minimum value is equal to 0 and the maximum value is equal to 1.","metadata":{}},{"cell_type":"markdown","source":"I said that the data to be used in the regression model should be numerical. When we look at the types of variables with \"dtypes\", we see that they are float.","metadata":{}},{"cell_type":"code","source":"knn_regression_data.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_independent = knn_regression_data.drop(\"displacement\", axis = 1)\nknn_dependent = knn_regression_data[\"displacement\"] # I want estimate to acceleration","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nindependent_train, independent_test, dependent_train, dependent_test = train_test_split(\n    knn_independent, \n    knn_dependent, \n    test_size = 0.10, \n    random_state = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nknn_model = KNeighborsRegressor().fit(independent_train, dependent_train)\npredicted_values = knn_model.predict(independent_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df = pd.DataFrame({\"Dependent_Test\" : dependent_test, \"Dependent_Predicted\" : predicted_values})\npredict_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have normalized the data before and gave values in the range of 0-1. I applied the reverse of the normalization process to see the real predictions with the code below.","metadata":{}},{"cell_type":"code","source":"predict_df = (predict_df*(np.max(cars_data.displacement) - np.min(cars_data.displacement))) + np.min(cars_data.displacement)\npredict_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, r2_score\nimport numpy as np\n\nprint(\"Mean Squared Error = \", mean_squared_error(predict_df.Dependent_Predicted, predict_df.Dependent_Test))\nprint(\"Root Mean Squared Error = \", np.sqrt(mean_squared_error(predict_df.Dependent_Predicted, predict_df.Dependent_Test)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we want to examine the success of the model with statistical methods, we can look at the MSE value. In statistics, the mean squared error (MSE) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.","metadata":{}},{"cell_type":"markdown","source":"![](https://veribilimcisi.files.wordpress.com/2017/07/83buy.png)","metadata":{}},{"cell_type":"code","source":"r2_score(predict_df.Dependent_Predicted, predict_df.Dependent_Test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"R-squared values range from 0 to 1 and are commonly stated as percentages from 0% to 100%. An R-squared of 100% means that all dependent variables are completely explained by movements in the index (or the independent variable(s) you are interested in).","metadata":{}},{"cell_type":"markdown","source":"## Parameter Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport numpy as np\n\nknn_params = {\"n_neighbors\" : np.arange(1,11,1)}\nknn = KNeighborsRegressor()\nknn_cv_model = GridSearchCV(knn, knn_params, cv = 10)\nknn_cv_model.fit(independent_train, dependent_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"knn_cv_model.best_params_[\"n_neighbors\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As a result of the Parameter Tuning process, we determined that the optimum neighbor number (k) is 9.","metadata":{}},{"cell_type":"code","source":"knn_model = KNeighborsRegressor(n_neighbors = knn_cv_model.best_params_[\"n_neighbors\"]).fit(independent_train, dependent_train)\npredicted_values = knn_model.predict(independent_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df = pd.DataFrame({\"Dependent_Test\" : dependent_test, \"Dependent_Predicted\" : predicted_values})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_df = (predict_df*(np.max(cars_data.displacement) - np.min(cars_data.displacement))) + np.min(cars_data.displacement)\npredict_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Mean Squared Error = \", mean_squared_error(predict_df.Dependent_Test, predict_df.Dependent_Predicted))\nprint(\"Root Mean Squared Error = \", np.sqrt(mean_squared_error(predict_df.Dependent_Test, predict_df.Dependent_Predicted)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the mse value decreases in the use of optimum parameters.","metadata":{}},{"cell_type":"code","source":"r2_score(predict_df.Dependent_Test, predict_df.Dependent_Predicted)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nMSE = []\nMSE_CV = []\n\nfor k in range(10):\n    k = k + 1\n    knn_model = KNeighborsRegressor(n_neighbors = k).fit(independent_train, dependent_train)\n    y_pred = knn_model.predict(independent_test)\n    mse = mean_squared_error(y_pred, dependent_test)\n    mse_cv = -1 * cross_val_score(knn_model, independent_train,dependent_train, cv = 10,\n                         scoring = \"neg_mean_squared_error\").mean()\n    MSE.append(mse)\n    MSE_CV.append(mse_cv)\n    print(\"k =\", k, \"MSE :\", mse, \"MSE_CV:\", mse_cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(np.arange(1,11,1), MSE)\nplt.plot(np.arange(1,11,1), MSE_CV)\nplt.xlabel(\"Value of K for KNN\")\nplt.ylabel(\"Testing Accurracy\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}